{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "b2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_fT2N6grzZT",
        "outputId": "b1c0d6d4-c8a5-4204-e53f-4360505af854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 10.1 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 67.9 MB/s \n",
            "\u001b[?25hCollecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.4-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[K     |████████████████████████████████| 143 kB 40.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=f9fe81a8c6bc3fe13508a4337dafd6647658d4ea836040f8ebe7c627a844a2f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb, box2d\n",
            "Successfully installed GitPython-3.1.26 box2d-2.3.10 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.4 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.10 yaspin-2.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install box2d wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Neural Networks\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
        "from tensorflow.keras.regularizers import *\n",
        "\n",
        "# Reinforcement Learning\n",
        "import gym\n",
        "\n",
        "# Model Tracking\n",
        "import wandb\n",
        "\n",
        "# Data Structures\n",
        "from collections import deque\n",
        "\n",
        "# Miscellaneous\n",
        "import os\n",
        "import pickle"
      ],
      "metadata": {
        "id": "t9bmn2T0r-Iy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ARTIFACT_DIRECTORY = 'drive/MyDrive/dqn0/'\n",
        "ENV = gym.make('LunarLander-v2')\n",
        "AGENT_CONFIG = {\n",
        "    'batch_size': 64,\n",
        "    'epsilon': 1.0,\n",
        "    'min_epsilon': 0.01,\n",
        "    'epsilon_decay': 0.995,\n",
        "    'gamma': 0.99,\n",
        "    'alpha': 0.0005\n",
        "}"
      ],
      "metadata": {
        "id": "e_HZMwB-u5zP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_str = '_'.join((f'{k}={v}' for k, v in AGENT_CONFIG.items())) + '.log'\n",
        "config_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jokJRqwxvrac",
        "outputId": "fac29a62-7098-4023-bf24-136f79858d9a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'batch_size=64_epsilon=1.0_min_epsilon=0.01_epsilon_decay=0.995_gamma=0.99_alpha=0.0005.log'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_length: int):\n",
        "        self.max_length = max_length\n",
        "        self.buffer = [None] * self.max_length\n",
        "        self.pointer = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, memory):\n",
        "        if self.pointer >= self.max_length:\n",
        "            self.pointer = 0\n",
        "        elif self.size < self.max_length:\n",
        "            self.size += 1\n",
        "        self.buffer[self.pointer] = memory\n",
        "        self.pointer += 1\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        return [self.buffer[i] for i in np.random.randint(0, self.size, batch_size)]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.buffer[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.size"
      ],
      "metadata": {
        "id": "1tmRuKSjsIfO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FileLogger:\n",
        "    def __init__(self, log_file: str, *args, sep: str=';'):\n",
        "        self.log_file = log_file\n",
        "        self.sep = sep\n",
        "        self.n_args = len(args)\n",
        "        if not os.path.exists(log_file):\n",
        "            self.setup(args)\n",
        "    \n",
        "    def setup(self, args):\n",
        "        header = self.sep.join(args)\n",
        "        with open(self.log_file, mode='w') as f:\n",
        "            f.write(header)\n",
        "            f.write('\\n')\n",
        "\n",
        "    def log(self, **kwargs):\n",
        "        assert len(kwargs) == self.n_args\n",
        "        header = ''\n",
        "        with open(self.log_file, mode='r') as f:\n",
        "            header = f.readlines()[0].replace('\\n', '')\n",
        "        params = header.split(self.sep)\n",
        "        values = (str(kwargs[p.lower()]) for p in params)\n",
        "        entry = self.sep.join(values)\n",
        "        with open(self.log_file, mode='a+') as f:\n",
        "            f.write(entry)\n",
        "            f.write('\\n')\n",
        "        return self\n",
        "    \n",
        "    def get_logs(self):\n",
        "        df = pd.read_csv(self.log_file, header=0, sep=self.sep)\n",
        "        past_rewards = df['Total_Rewards']\n",
        "        return past_rewards"
      ],
      "metadata": {
        "id": "2PI8aDaVuuXy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardTracker(deque):\n",
        "    def __init__(self, max_length: int):\n",
        "        super().__init__(maxlen=max_length)\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def log(self, total_episodic_rewards):\n",
        "        self.append(total_episodic_rewards)\n",
        "    \n",
        "    def get_rolling_mean(self):\n",
        "        base = min(self.__len__(), self.max_length)\n",
        "        return sum((r for r in self)) / float(base)"
      ],
      "metadata": {
        "id": "FoFZTUNwuzKL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DQN(input_shape, output_shape, alpha):\n",
        "    model = Sequential(layers=[\n",
        "        Dense(256, input_shape=input_shape, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(output_shape)\n",
        "    ])\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=alpha))\n",
        "    return model"
      ],
      "metadata": {
        "id": "toNtKgLbsSQP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 batch_size: int,\n",
        "                 alpha: float,\n",
        "                 epsilon: float,\n",
        "                 min_epsilon: float,\n",
        "                 epsilon_decay: float,\n",
        "                 gamma: float,\n",
        "                 max_memory_length: int = 300_000,\n",
        "                 checkpoint_interval: int = 10,\n",
        "                 log_file: str = 'progress.log'):\n",
        "        self.env = env\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.gamma = gamma\n",
        "        self.replay_buffer = ReplayBuffer(max_length = max_memory_length)\n",
        "        self.model = DQN(input_shape=env.observation_space.shape,\n",
        "                         output_shape=env.action_space.n,\n",
        "                         alpha=alpha)\n",
        "        self.file_logger = FileLogger(ARTIFACT_DIRECTORY + log_file, 'Episode', 'Steps', 'Total_Rewards')\n",
        "        self.checkpoint_interval = checkpoint_interval\n",
        "    \n",
        "    def checkpoint(self, episode, steps, total_rewards):\n",
        "        self.file_logger.log(episode=episode, steps=steps, total_rewards=total_rewards)\n",
        "        if episode % self.checkpoint_interval == 0:\n",
        "            self.model.save(ARTIFACT_DIRECTORY + f'{episode}.h5')\n",
        "            with open(f'{ARTIFACT_DIRECTORY}replay_buffer-{episode}.p', 'wb') as saved_buffer:\n",
        "                pickle.dump(self.replay_buffer, saved_buffer)\n",
        "    \n",
        "    def act(self, state):\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        return np.argmax(self.model.predict(state.reshape(1, -1))[0])\n",
        "    \n",
        "    def calculate_target_values(self, memory_batch):\n",
        "        states = []\n",
        "        next_states = []\n",
        "        for sample in memory_batch:\n",
        "            states.append(sample[0])\n",
        "            next_states.append(sample[3])\n",
        "        states = np.array(states)\n",
        "        next_states = np.array(next_states)\n",
        "\n",
        "        q_values = self.model.predict(states)\n",
        "        # target_q_values = self.target_model.predict(states)\n",
        "\n",
        "        q_values_next_state = self.model.predict(next_states)\n",
        "        # target_q_values_next_state = self.target_model.predict(next_states)\n",
        "\n",
        "        q_copy = q_values.copy()\n",
        "\n",
        "        # targets = []\n",
        "        for index, (s, a, r, s_, d) in enumerate(memory_batch):\n",
        "            best_action = np.max(q_values_next_state[index])\n",
        "            # best_action_next_state_q_value = q_[index][best_action]\n",
        "            \n",
        "            # target_vector = [0, 0, 0, 0]\n",
        "            q_copy[index, a] = r if d else r + self.gamma * best_action\n",
        "            # targets.append(target_vector)\n",
        "\n",
        "        return states, q_copy\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay_buffer) >= self.batch_size:\n",
        "            samples = self.replay_buffer.sample(batch_size=self.batch_size)\n",
        "            states, targets = self.calculate_target_values(samples)\n",
        "            self.model.fit(states, targets, epochs=1, batch_size=targets.shape[0], verbose=0)\n",
        "            self.decay_epsilon()\n",
        "    \n",
        "    # def transfer_weights(self):\n",
        "    #     policy_weights = self.model.get_weights()\n",
        "    #     target_weights = self.target_model.get_weights()\n",
        "    #     if self.tau < 1.0:\n",
        "    #         for i, weights in enumerate(policy_weights):\n",
        "    #             target_weights[i] = weights * self.tau + target_weights[i] * (1 - self.tau)\n",
        "    #     self.target_model.set_weights(target_weights)\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)"
      ],
      "metadata": {
        "id": "VjBrC6DbssBw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"lunarlander-dqn\",\n",
        "           entity=\"ethanolx\",\n",
        "           config=AGENT_CONFIG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Q3ApqJgivab3",
        "outputId": "425bf4b3-3bd2-418a-982d-ae21272bc390"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33methanolx\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/ethanolx/lunarlander-dqn/runs/1sxunxyn\" target=\"_blank\">exalted-gorge-32</a></strong> to <a href=\"https://wandb.ai/ethanolx/lunarlander-dqn\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fedd41acf10>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/ethanolx/lunarlander-dqn/runs/1sxunxyn?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(warm_start: int = 0):\n",
        "    episodes = 1000\n",
        "    # max_steps_per_episode = 800\n",
        "    # target_model_train_interval = 1\n",
        "    reward_progress = RewardTracker(max_length=100)\n",
        "    dqn_agent = Agent(env=ENV, **AGENT_CONFIG, log_file=config_str)\n",
        "    if warm_start >= 1:\n",
        "        print(f'Resuming from last episode: {warm_start}')\n",
        "        dqn_agent.epsilon = dqn_agent.min_epsilon\n",
        "        # dqn_agent.target_model = load_model(f'{ARTIFACT_DIRECTORY}{warm_start}.h5')\n",
        "        dqn_agent.model = load_model(f'{ARTIFACT_DIRECTORY}{warm_start}.h5')\n",
        "        with open(f'{ARTIFACT_DIRECTORY}replay_buffer-{warm_start}.p', 'rb') as saved_buffer:\n",
        "            dqn_agent.replay_buffer = pickle.load(saved_buffer)\n",
        "        for r in dqn_agent.file_logger.get_logs():\n",
        "            reward_progress.append(r)\n",
        "    for episode in range(warm_start + 1, episodes + warm_start + 1):\n",
        "        cur_state = ENV.reset()\n",
        "        total_episode_rewards = 0\n",
        "        done = False\n",
        "        step = 0\n",
        "        while not done:\n",
        "            action = dqn_agent.act(cur_state)\n",
        "            new_state, reward, done, _ = ENV.step(action)\n",
        "\n",
        "            total_episode_rewards += reward\n",
        "            print(f'\\rEpisode: {episode}; Step: {step}; Reward: {reward}; Total Episode Rewards (thus far): {total_episode_rewards}', end='')\n",
        "\n",
        "            dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
        "            dqn_agent.learn()\n",
        "            cur_state = new_state\n",
        "            if done or total_episode_rewards < -300:\n",
        "                break\n",
        "            step += 1\n",
        "\n",
        "        print()\n",
        "        reward_progress.append(total_episode_rewards)\n",
        "        print(f'Rolling Mean: {reward_progress.get_rolling_mean()}')\n",
        "        dqn_agent.checkpoint(episode=episode, steps=step, total_rewards=total_episode_rewards)\n",
        "        # dqn_agent.decay_epsilon()\n",
        "        if total_episode_rewards < 200:\n",
        "            print(\"Episode failed\")\n",
        "        else:\n",
        "            print(f\"Completed in {episode} episodes\")\n",
        "        print(f'New Epsilon: {dqn_agent.epsilon}')\n",
        "        wandb.log({\n",
        "            'episode': episode,\n",
        "            'steps': step,\n",
        "            'total rewards': total_episode_rewards\n",
        "        })"
      ],
      "metadata": {
        "id": "u7AvPp51uqyz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(warm_start=320)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpJ6AOREvuDC",
        "outputId": "c7d5bcda-68fe-43da-9fbd-667040d88189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from last episode: 320\n",
            "Episode: 321; Step: 267; Reward: 100; Total Episode Rewards (thus far): 253.58655207053533\n",
            "Rolling Mean: 200.41095640376713\n",
            "Completed in 321 episodes\n",
            "New Epsilon: 0.01\n",
            "Episode: 322; Step: 289; Reward: 100; Total Episode Rewards (thus far): 286.3541215788838\n",
            "Rolling Mean: 201.85619206554358\n",
            "Completed in 322 episodes\n",
            "New Epsilon: 0.01\n",
            "Episode: 323; Step: 262; Reward: 100; Total Episode Rewards (thus far): 312.2952204413689\n",
            "Rolling Mean: 202.56130980592877\n",
            "Completed in 323 episodes\n",
            "New Epsilon: 0.01\n",
            "Episode: 324; Step: 259; Reward: -0.21635450843138557; Total Episode Rewards (thus far): 146.22484068875045"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "cur5G2YpvuwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez-setup > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install box2d-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "caMcX-tAtuDH",
        "outputId": "7f6e026f-c73a-4253-82b9-bf60da94c6b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-60.8.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 14.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed setuptools-60.8.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-2.2-py3-none-any.whl (15 kB)\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-1.1 pyvirtualdisplay-2.2\n",
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 14.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "Box2D"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r drive/MyDrive/dqn0/video\n",
        "!cd drive/MyDrive/dqn0;xvfb-run -a -s \"-screen 0 640x480x24\" python make_video.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV7pSHsQ3Zoj",
        "outputId": "97326e3e-e34f-49e1-b8f0-65329fbdbe18"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'drive/MyDrive/dqn0/video': No such file or directory\n",
            "2022-02-10 14:28:34.872586: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        }
      ]
    }
  ]
}