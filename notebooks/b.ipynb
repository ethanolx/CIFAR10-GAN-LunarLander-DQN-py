{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfzQ-EPaQ8I4",
        "outputId": "b8c90521-20c9-427e-a96a-e047a4532451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 12.3 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 39.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 49.8 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.4-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[K     |████████████████████████████████| 143 kB 50.4 MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=6bdc51f2cd9432a3d0adbd7f4730d9ba2117d043739fa3cb11cf7e720f155640\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=af3fe3609a0ab53b46f10bb09991b050d05405ace8fc569f78c9afae8311886f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb, box2d\n",
            "Successfully installed GitPython-3.1.26 box2d-2.3.10 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.4 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.9 yaspin-2.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install box2d wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsp5j7eJh_2N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.optimizers import *\n",
        "import gym\n",
        "import wandb\n",
        "from collections import deque\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oPZkoeYuRVj"
      },
      "outputs": [],
      "source": [
        "ARTIFACT_DIRECTORY = 'drive/MyDrive/rl/dqn/'\n",
        "ENV = gym.make('LunarLander-v2')\n",
        "CONFIG = {\n",
        "    'batch_size': 128,\n",
        "    'alpha': 0.001,\n",
        "    'epsilon': 1.0,\n",
        "    'min_epsilon': 0.01,\n",
        "    'epsilon_decay': 0.995,\n",
        "    'gamma': 0.95,\n",
        "    'tau': 1.,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cq6i_ypXyfPK",
        "outputId": "8ffaafc2-6013-438b-d685-9419ca2162d1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'batch_size=128_alpha=0.001_epsilon=1.0_min_epsilon=0.01_epsilon_decay=0.995_gamma=0.95_tau=1.0.log'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config_str = '_'.join((f'{k}={v}' for k, v in CONFIG.items())) + '.log'\n",
        "config_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "PpFjcOX4kUhb",
        "outputId": "ff75b81a-08d7-4ae5-aaf4-9af56992d190"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/ethanolx/lunarlander-dqn/runs/2wx5gjih\" target=\"_blank\">chromatic-goat-15</a></strong> to <a href=\"https://wandb.ai/ethanolx/lunarlander-dqn\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/ethanolx/lunarlander-dqn/runs/2wx5gjih?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7feea8991490>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# wandb.login()\n",
        "wandb.init(project=\"lunarlander-dqn\",\n",
        "           entity=\"ethanolx\",\n",
        "           config=CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9gprhW_v8NY",
        "outputId": "c137175f-9968-462e-d0fd-ac1df748984f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8,)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ENV.observation_space.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQmlXKgiv_AB",
        "outputId": "e7f296d4-d03b-4c05-f365-7ca2ddca7483"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ENV.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McuFI9xEhH2E"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_length: int):\n",
        "        self.max_length = max_length\n",
        "        self.buffer = [None] * self.max_length\n",
        "        self.pointer = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, memory):\n",
        "        if self.pointer >= self.max_length:\n",
        "            self.pointer = 0\n",
        "        else:\n",
        "            self.size += 1\n",
        "        self.buffer[self.pointer] = memory\n",
        "        self.pointer += 1\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        return [self.buffer[i] for i in np.random.randint(0, self.size, batch_size)]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.buffer[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa2Qwz3hvQvc"
      },
      "outputs": [],
      "source": [
        "class RewardTracker(deque):\n",
        "    def __init__(self, max_length: int):\n",
        "        super().__init__(maxlen=max_length)\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def log(self, total_episodic_rewards):\n",
        "        self.append(total_episodic_rewards)\n",
        "    \n",
        "    def get_rolling_mean(self):\n",
        "        base = min(self.__len__(), self.max_length)\n",
        "        return sum((r for r in self)) / float(base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq_AujVYjyH6"
      },
      "outputs": [],
      "source": [
        "def DQN(input_shape, output_shape, alpha: float):\n",
        "    model = Sequential(layers=[\n",
        "        Dense(128, input_shape=input_shape, activation='relu', kernel_regularizer='l1'),\n",
        "        BatchNormalization(),\n",
        "        Dense(128, activation='relu', kernel_regularizer='l1'),\n",
        "        BatchNormalization(),\n",
        "        Dense(128, activation='relu', kernel_regularizer='l1'),\n",
        "        Dropout(rate=0.2),\n",
        "        Dense(output_shape)\n",
        "    ])\n",
        "    model.compile(loss='mae', optimizer=Adam(learning_rate=alpha))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-lP4_VRj6tj"
      },
      "outputs": [],
      "source": [
        "class FileLogger:\n",
        "    def __init__(self, log_file: str, *args, sep: str=';'):\n",
        "        self.log_file = log_file\n",
        "        self.sep = sep\n",
        "        self.n_args = len(args)\n",
        "        if not os.path.exists(log_file):\n",
        "            self.setup(args)\n",
        "    \n",
        "    def setup(self, args):\n",
        "        header = self.sep.join(args)\n",
        "        with open(self.log_file, mode='w') as f:\n",
        "            f.write(header)\n",
        "            f.write('\\n')\n",
        "\n",
        "    def log(self, **kwargs):\n",
        "        assert len(kwargs) == self.n_args\n",
        "        header = ''\n",
        "        with open(self.log_file, mode='r') as f:\n",
        "            header = f.readlines()[0].replace('\\n', '')\n",
        "        params = header.split(self.sep)\n",
        "        values = (str(kwargs[p.lower()]) for p in params)\n",
        "        entry = self.sep.join(values)\n",
        "        with open(self.log_file, mode='a+') as f:\n",
        "            f.write(entry)\n",
        "            f.write('\\n')\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqogW6_Qj3n8"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 batch_size: int,\n",
        "                 alpha: float,\n",
        "                 epsilon: float,\n",
        "                 min_epsilon: float,\n",
        "                 epsilon_decay: float,\n",
        "                 gamma: float,\n",
        "                 tau: float,\n",
        "                 max_memory_length: int = 100_000,\n",
        "                 log_file: str = 'progress.log'):\n",
        "        self.env = env\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.replay_buffer = ReplayBuffer(max_length = max_memory_length)\n",
        "        self.model = DQN(input_shape=env.observation_space.shape,\n",
        "                         output_shape=env.action_space.n,\n",
        "                         alpha=alpha)\n",
        "        self.target_model = DQN(input_shape=env.observation_space.shape,\n",
        "                         output_shape=env.action_space.n,\n",
        "                         alpha=alpha)\n",
        "        self.file_logger = FileLogger(ARTIFACT_DIRECTORY + log_file, 'Episode', 'Steps', 'Total_Rewards')\n",
        "    \n",
        "    def checkpoint(self, episode, steps, total_rewards):\n",
        "        self.file_logger.log(episode=episode, steps=steps, total_rewards=total_rewards)\n",
        "        self.model.save(ARTIFACT_DIRECTORY + f'{episode}.h5')\n",
        "    \n",
        "    def act(self, state):\n",
        "        if np.random.uniform(0, 1, 1) < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        return np.argmax(self.model.predict(state.reshape(1, -1))[0])\n",
        "    \n",
        "    def calculate_target_values(self, memory_batch):\n",
        "        states = []\n",
        "        next_states = []\n",
        "        for sample in memory_batch:\n",
        "            states.append(sample[0])\n",
        "            next_states.append(sample[3])\n",
        "        states = np.array(states)\n",
        "        next_states = np.array(next_states)\n",
        "\n",
        "        q_values = self.model.predict(states)\n",
        "        target_q_values = self.target_model.predict(states)\n",
        "\n",
        "        q_values_next_state = self.model.predict(next_states)\n",
        "        target_q_values_next_state = self.target_model.predict(next_states)\n",
        "\n",
        "        targets = []\n",
        "        for index, (s, a, r, s_, d) in enumerate(memory_batch):\n",
        "            best_action = np.argmax(q_values_next_state[index])\n",
        "            best_action_next_state_q_value = target_q_values_next_state[index][best_action]\n",
        "            \n",
        "            target_vector = [0, 0, 0, 0]\n",
        "            target_vector[a] = r if d else r + self.gamma * best_action_next_state_q_value\n",
        "            targets.append(target_vector)\n",
        "\n",
        "        return states, np.array(targets)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay_buffer) >= self.batch_size:\n",
        "            samples = self.replay_buffer.sample(batch_size=self.batch_size)\n",
        "            states, targets = self.calculate_target_values(samples)\n",
        "            self.model.fit(states, targets, epochs=1, verbose=0)\n",
        "    \n",
        "    def transfer_weights(self):\n",
        "        policy_weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        if self.tau < 1.0:\n",
        "            for i, weights in enumerate(policy_weights):\n",
        "                target_weights[i] = weights * self.tau + target_weights[i] * (1 - self.tau)\n",
        "        self.target_model.set_weights(target_weights)\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH_nwv1rvBWu"
      },
      "outputs": [],
      "source": [
        "def main(warm_start: int = 0, checkpoint_interval: int = 50):\n",
        "    episodes = 1000\n",
        "    max_steps_per_episode = 500\n",
        "    target_model_train_interval = 10\n",
        "    reward_progress = RewardTracker(max_length=100)\n",
        "    dqn_agent = Agent(env=ENV, **CONFIG, log_file=config_str)\n",
        "    if warm_start >= 1:\n",
        "        print(f'Resuming from last episode: {warm_start}')\n",
        "        dqn_agent.epsilon = max(dqn_agent.min_epsilon, dqn_agent.epsilon_decay ** (warm_start - 1))\n",
        "        dqn_agent.target_model = load_model(f'{ARTIFACT_DIRECTORY}{warm_start}.h5')\n",
        "        dqn_agent.model = load_model(f'{ARTIFACT_DIRECTORY}{warm_start}.h5')\n",
        "    for episode in range(warm_start + 1, episodes + warm_start + 1):\n",
        "        cur_state = ENV.reset()\n",
        "        total_episode_rewards = 0\n",
        "        for step in range(max_steps_per_episode):\n",
        "            action = dqn_agent.act(cur_state)\n",
        "            new_state, reward, done, _ = ENV.step(action)\n",
        "\n",
        "            total_episode_rewards += reward\n",
        "            print(f'\\rEpisode: {episode}; Step: {step}; Reward: {reward}; Total Episode Rewards (thus far): {total_episode_rewards}', end='')\n",
        "\n",
        "            dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
        "            dqn_agent.learn()\n",
        "            cur_state = new_state\n",
        "            if done:\n",
        "                break\n",
        "        else:\n",
        "            print(f'\\nFailed to complete episode {episode} within {max_steps_per_episode} steps')\n",
        "        print()\n",
        "        if episode % target_model_train_interval == 0:\n",
        "            dqn_agent.transfer_weights()\n",
        "        reward_progress.append(total_episode_rewards)\n",
        "        print(f'Rolling Mean: {reward_progress.get_rolling_mean()}')\n",
        "        if episode % checkpoint_interval == 0:\n",
        "            dqn_agent.checkpoint(episode=episode, steps=step, total_rewards=total_episode_rewards)\n",
        "        if total_episode_rewards < 200:\n",
        "            print(\"Episode failed\")\n",
        "        else:\n",
        "            print(f\"Completed in {episode} episodes\")\n",
        "        dqn_agent.decay_epsilon()\n",
        "        print(f'New Epsilon: {dqn_agent.epsilon}')\n",
        "        wandb.log({\n",
        "            'episode': episode,\n",
        "            'steps': step,\n",
        "            'total rewards': total_episode_rewards\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GiPdh_bkk2OE",
        "outputId": "e4760e27-b680-45ec-bdc9-a64d0b831720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from last episode: 100\n",
            "Episode: 101; Step: 140; Reward: -100; Total Episode Rewards (thus far): -355.6257529792642\n",
            "Rolling Mean: -355.6257529792642\n",
            "Episode failed\n",
            "New Epsilon: 0.6057704364907279\n",
            "Episode: 102; Step: 143; Reward: -100; Total Episode Rewards (thus far): -200.65385316922993\n",
            "Rolling Mean: -278.1398030742471\n",
            "Episode failed\n",
            "New Epsilon: 0.6027415843082743\n",
            "Episode: 103; Step: 131; Reward: -100; Total Episode Rewards (thus far): -186.56575113312152\n",
            "Rolling Mean: -247.61511909387187\n",
            "Episode failed\n",
            "New Epsilon: 0.599727876386733\n",
            "Episode: 104; Step: 157; Reward: -100; Total Episode Rewards (thus far): -101.3384995341242\n",
            "Rolling Mean: -211.04596420393494\n",
            "Episode failed\n",
            "New Epsilon: 0.5967292370047993\n",
            "Episode: 105; Step: 143; Reward: -100; Total Episode Rewards (thus far): -178.05746983908978\n",
            "Rolling Mean: -204.4482653309659\n",
            "Episode failed\n",
            "New Epsilon: 0.5937455908197753\n",
            "Episode: 106; Step: 91; Reward: -100; Total Episode Rewards (thus far): -244.96103627052645\n",
            "Rolling Mean: -211.20039382089269\n",
            "Episode failed\n",
            "New Epsilon: 0.5907768628656764\n",
            "Episode: 107; Step: 103; Reward: -100; Total Episode Rewards (thus far): -253.1169298387859\n",
            "Rolling Mean: -217.1884703948774\n",
            "Episode failed\n",
            "New Epsilon: 0.587822978551348\n",
            "Episode: 108; Step: 150; Reward: -100; Total Episode Rewards (thus far): -570.9353494650659\n",
            "Rolling Mean: -261.406830278651\n",
            "Episode failed\n",
            "New Epsilon: 0.5848838636585912\n",
            "Episode: 109; Step: 122; Reward: -100; Total Episode Rewards (thus far): -282.5403989868506\n",
            "Rolling Mean: -263.75500457956207\n",
            "Episode failed\n",
            "New Epsilon: 0.5819594443402983\n",
            "Episode: 110; Step: 120; Reward: -100; Total Episode Rewards (thus far): -102.15565992361006\n",
            "Rolling Mean: -247.59507011396687\n",
            "Episode failed\n",
            "New Epsilon: 0.5790496471185969\n",
            "Episode: 111; Step: 120; Reward: -100; Total Episode Rewards (thus far): -192.65028823315603\n",
            "Rolling Mean: -242.60008994298406\n",
            "Episode failed\n",
            "New Epsilon: 0.5761543988830039\n",
            "Episode: 112; Step: 202; Reward: -100; Total Episode Rewards (thus far): -57.78553007037269\n",
            "Rolling Mean: -227.19887662026645\n",
            "Episode failed\n",
            "New Epsilon: 0.5732736268885888\n",
            "Episode: 113; Step: 179; Reward: -100; Total Episode Rewards (thus far): -37.24129269414054\n",
            "Rolling Mean: -212.58675477979523\n",
            "Episode failed\n",
            "New Epsilon: 0.5704072587541459\n",
            "Episode: 114; Step: 102; Reward: -100; Total Episode Rewards (thus far): -213.5586865498509\n",
            "Rolling Mean: -212.65617847765634\n",
            "Episode failed\n",
            "New Epsilon: 0.5675552224603752\n",
            "Episode: 115; Step: 130; Reward: -100; Total Episode Rewards (thus far): -521.9643121619035\n",
            "Rolling Mean: -233.27672072327283\n",
            "Episode failed\n",
            "New Epsilon: 0.5647174463480733\n",
            "Episode: 116; Step: 135; Reward: -100; Total Episode Rewards (thus far): -355.23531950955385\n",
            "Rolling Mean: -240.8991331474154\n",
            "Episode failed\n",
            "New Epsilon: 0.5618938591163329\n",
            "Episode: 117; Step: 133; Reward: -100; Total Episode Rewards (thus far): -57.040190865199875\n",
            "Rolling Mean: -230.08390124846153\n",
            "Episode failed\n",
            "New Epsilon: 0.5590843898207513\n",
            "Episode: 118; Step: 151; Reward: -100; Total Episode Rewards (thus far): -225.46402359184043\n",
            "Rolling Mean: -229.82724137864923\n",
            "Episode failed\n",
            "New Epsilon: 0.5562889678716475\n",
            "Episode: 119; Step: 167; Reward: -100; Total Episode Rewards (thus far): -29.767830512148365\n",
            "Rolling Mean: -219.29779870146496\n",
            "Episode failed\n",
            "New Epsilon: 0.5535075230322892\n",
            "Episode: 120; Step: 119; Reward: -100; Total Episode Rewards (thus far): -187.8730174639142\n",
            "Rolling Mean: -217.7265596395874\n",
            "Episode failed\n",
            "New Epsilon: 0.5507399854171278\n",
            "Episode: 121; Step: 150; Reward: -100; Total Episode Rewards (thus far): -13.870846361306576\n",
            "Rolling Mean: -208.01914472157404\n",
            "Episode failed\n",
            "New Epsilon: 0.5479862854900421\n",
            "Episode: 122; Step: 108; Reward: -100; Total Episode Rewards (thus far): -170.5018017433971\n",
            "Rolling Mean: -206.3138109498387\n",
            "Episode failed\n",
            "New Epsilon: 0.5452463540625919\n",
            "Episode: 123; Step: 93; Reward: -100; Total Episode Rewards (thus far): -125.87285198328273\n",
            "Rolling Mean: -202.81637795129282\n",
            "Episode failed\n",
            "New Epsilon: 0.542520122292279\n",
            "Episode: 124; Step: 168; Reward: -100; Total Episode Rewards (thus far): -119.61133102600554\n",
            "Rolling Mean: -199.3495009960725\n",
            "Episode failed\n",
            "New Epsilon: 0.5398075216808176\n",
            "Episode: 125; Step: 126; Reward: -100; Total Episode Rewards (thus far): -133.95392405080048\n",
            "Rolling Mean: -196.7336779182616\n",
            "Episode failed\n",
            "New Epsilon: 0.5371084840724135\n",
            "Episode: 126; Step: 176; Reward: -100; Total Episode Rewards (thus far): -151.66966338954745\n",
            "Rolling Mean: -195.00044659023413\n",
            "Episode failed\n",
            "New Epsilon: 0.5344229416520514\n",
            "Episode: 127; Step: 89; Reward: -100; Total Episode Rewards (thus far): -309.1730482533045\n",
            "Rolling Mean: -199.22906146664414\n",
            "Episode failed\n",
            "New Epsilon: 0.5317508269437912\n",
            "Episode: 128; Step: 145; Reward: -100; Total Episode Rewards (thus far): -76.73624237537197\n",
            "Rolling Mean: -194.85431792767014\n",
            "Episode failed\n",
            "New Epsilon: 0.5290920728090722\n",
            "Episode: 129; Step: 154; Reward: -100; Total Episode Rewards (thus far): -60.748647701260026\n",
            "Rolling Mean: -190.22998447158704\n",
            "Episode failed\n",
            "New Epsilon: 0.5264466124450269\n",
            "Episode: 130; Step: 258; Reward: -100; Total Episode Rewards (thus far): -55.07760457090742\n",
            "Rolling Mean: -185.72490514156436\n",
            "Episode failed\n",
            "New Epsilon: 0.5238143793828017\n",
            "Episode: 131; Step: 165; Reward: -100; Total Episode Rewards (thus far): -283.76966500572917\n",
            "Rolling Mean: -188.88763933073096\n",
            "Episode failed\n",
            "New Epsilon: 0.5211953074858877\n",
            "Episode: 132; Step: 147; Reward: -100; Total Episode Rewards (thus far): -177.7104625377546\n",
            "Rolling Mean: -188.53835255595044\n",
            "Episode failed\n",
            "New Epsilon: 0.5185893309484583\n",
            "Episode: 133; Step: 151; Reward: -100; Total Episode Rewards (thus far): -311.97797989051566\n",
            "Rolling Mean: -192.27894732366454\n",
            "Episode failed\n",
            "New Epsilon: 0.515996384293716\n",
            "Episode: 134; Step: 219; Reward: -100; Total Episode Rewards (thus far): -214.3496533759122\n",
            "Rolling Mean: -192.92808573696595\n",
            "Episode failed\n",
            "New Epsilon: 0.5134164023722474\n",
            "Episode: 135; Step: 130; Reward: -100; Total Episode Rewards (thus far): -156.9172532633104\n",
            "Rolling Mean: -191.89920480914722\n",
            "Episode failed\n",
            "New Epsilon: 0.5108493203603861\n",
            "Episode: 136; Step: 149; Reward: -100; Total Episode Rewards (thus far): -166.35971769212813\n",
            "Rolling Mean: -191.18977461145224\n",
            "Episode failed\n",
            "New Epsilon: 0.5082950737585842\n",
            "Episode: 137; Step: 143; Reward: -100; Total Episode Rewards (thus far): -64.56124454446267\n",
            "Rolling Mean: -187.76738190693902\n",
            "Episode failed\n",
            "New Epsilon: 0.5057535983897913\n",
            "Episode: 138; Step: 238; Reward: -100; Total Episode Rewards (thus far): -275.1690514079176\n",
            "Rolling Mean: -190.0674258411753\n",
            "Episode failed\n",
            "New Epsilon: 0.5032248303978423\n",
            "Episode: 139; Step: 147; Reward: -100; Total Episode Rewards (thus far): -75.34116756629783\n",
            "Rolling Mean: -187.12572691105024\n",
            "Episode failed\n",
            "New Epsilon: 0.5007087062458531\n",
            "Episode: 140; Step: 132; Reward: -100; Total Episode Rewards (thus far): -218.94483846154267\n",
            "Rolling Mean: -187.92120469981256\n",
            "Episode failed\n",
            "New Epsilon: 0.4982051627146238\n",
            "Episode: 141; Step: 135; Reward: -100; Total Episode Rewards (thus far): -199.47588543279662\n",
            "Rolling Mean: -188.20302618110483\n",
            "Episode failed\n",
            "New Epsilon: 0.49571413690105065\n",
            "Episode: 142; Step: 312; Reward: -100; Total Episode Rewards (thus far): -155.40750964369104\n",
            "Rolling Mean: -187.42218054926167\n",
            "Episode failed\n",
            "New Epsilon: 0.4932355662165454\n",
            "Episode: 143; Step: 123; Reward: -100; Total Episode Rewards (thus far): -200.74571577840493\n",
            "Rolling Mean: -187.73203020575335\n",
            "Episode failed\n",
            "New Epsilon: 0.4907693883854627\n",
            "Episode: 144; Step: 76; Reward: -100; Total Episode Rewards (thus far): -111.29254409186322\n",
            "Rolling Mean: -185.9947691577104\n",
            "Episode failed\n",
            "New Epsilon: 0.4883155414435354\n",
            "Episode: 145; Step: 206; Reward: -100; Total Episode Rewards (thus far): -236.8345163408933\n",
            "Rolling Mean: -187.12454131733668\n",
            "Episode failed\n",
            "New Epsilon: 0.4858739637363177\n",
            "Episode: 146; Step: 227; Reward: -100; Total Episode Rewards (thus far): -163.52487195003636\n",
            "Rolling Mean: -186.61150502674317\n",
            "Episode failed\n",
            "New Epsilon: 0.4834445939176361\n",
            "Episode: 147; Step: 180; Reward: -100; Total Episode Rewards (thus far): -117.01711546662915\n",
            "Rolling Mean: -185.13077333397482\n",
            "Episode failed\n",
            "New Epsilon: 0.4810273709480479\n",
            "Episode: 148; Step: 138; Reward: -100; Total Episode Rewards (thus far): -182.61420535965846\n",
            "Rolling Mean: -185.07834483450986\n",
            "Episode failed\n",
            "New Epsilon: 0.4786222340933077\n",
            "Episode: 149; Step: 499; Reward: -1.733212305641486; Total Episode Rewards (thus far): 60.446984553205226\n",
            "Failed to complete episode 149 within 500 steps\n",
            "\n",
            "Rolling Mean: -180.0676238265973\n",
            "Episode failed\n",
            "New Epsilon: 0.47622912292284114\n",
            "Episode: 150; Step: 263; Reward: -100; Total Episode Rewards (thus far): -48.14488679326897\n",
            "Rolling Mean: -177.42916908593074\n",
            "Episode failed\n",
            "New Epsilon: 0.4738479773082269\n",
            "Episode: 151; Step: 265; Reward: -100; Total Episode Rewards (thus far): -78.26536399644621\n",
            "Rolling Mean: -175.48478075084282\n",
            "Episode failed\n",
            "New Epsilon: 0.4714787374216858\n",
            "Episode: 152; Step: 140; Reward: -100; Total Episode Rewards (thus far): -213.42205115408007\n",
            "Rolling Mean: -176.21434364321277\n",
            "Episode failed\n",
            "New Epsilon: 0.46912134373457737\n",
            "Episode: 153; Step: 212; Reward: -100; Total Episode Rewards (thus far): -130.49312555108537\n",
            "Rolling Mean: -175.35167915090847\n",
            "Episode failed\n",
            "New Epsilon: 0.46677573701590447\n",
            "Episode: 154; Step: 117; Reward: -100; Total Episode Rewards (thus far): -77.30658670871712\n",
            "Rolling Mean: -173.5360292908679\n",
            "Episode failed\n",
            "New Epsilon: 0.46444185833082496\n",
            "Episode: 155; Step: 201; Reward: -100; Total Episode Rewards (thus far): -110.6372648313427\n",
            "Rolling Mean: -172.3924153916038\n",
            "Episode failed\n",
            "New Epsilon: 0.46211964903917085\n",
            "Episode: 156; Step: 191; Reward: -100; Total Episode Rewards (thus far): -97.97958313295925\n",
            "Rolling Mean: -171.06361481555658\n",
            "Episode failed\n",
            "New Epsilon: 0.459809050793975\n",
            "Episode: 157; Step: 111; Reward: -100; Total Episode Rewards (thus far): -105.50406997110711\n",
            "Rolling Mean: -169.91344736214518\n",
            "Episode failed\n",
            "New Epsilon: 0.4575100055400051\n",
            "Episode: 158; Step: 299; Reward: -100; Total Episode Rewards (thus far): -62.87916630673781\n",
            "Rolling Mean: -168.06802872325886\n",
            "Episode failed\n",
            "New Epsilon: 0.45522245551230506\n",
            "Episode: 159; Step: 145; Reward: -100; Total Episode Rewards (thus far): -193.84407141331604\n",
            "Rolling Mean: -168.50491080275134\n",
            "Episode failed\n",
            "New Epsilon: 0.45294634323474353\n",
            "Episode: 160; Step: 111; Reward: -100; Total Episode Rewards (thus far): -71.12730896296964\n",
            "Rolling Mean: -166.88195077208832\n",
            "Episode failed\n",
            "New Epsilon: 0.45068161151856984\n",
            "Episode: 161; Step: 139; Reward: -100; Total Episode Rewards (thus far): 5.223091783167803\n",
            "Rolling Mean: -164.06055663183824\n",
            "Episode failed\n",
            "New Epsilon: 0.448428203460977\n",
            "Episode: 162; Step: 182; Reward: -100; Total Episode Rewards (thus far): -219.99925547709182\n",
            "Rolling Mean: -164.9627937099875\n",
            "Episode failed\n",
            "New Epsilon: 0.4461860624436721\n",
            "Episode: 163; Step: 174; Reward: -100; Total Episode Rewards (thus far): -160.04594589433574\n",
            "Rolling Mean: -164.88474850656445\n",
            "Episode failed\n",
            "New Epsilon: 0.4439551321314537\n",
            "Episode: 164; Step: 164; Reward: -100; Total Episode Rewards (thus far): -41.64751735435084\n",
            "Rolling Mean: -162.9591667698111\n",
            "Episode failed\n",
            "New Epsilon: 0.4417353564707964\n",
            "Episode: 165; Step: 106; Reward: -100; Total Episode Rewards (thus far): -89.3470465127966\n",
            "Rolling Mean: -161.82667261201087\n",
            "Episode failed\n",
            "New Epsilon: 0.43952667968844245\n",
            "Episode: 166; Step: 123; Reward: -100; Total Episode Rewards (thus far): -134.06362951576267\n",
            "Rolling Mean: -161.4060204438859\n",
            "Episode failed\n",
            "New Epsilon: 0.43732904629000025\n",
            "Episode: 167; Step: 278; Reward: -100; Total Episode Rewards (thus far): -66.6341425452162\n",
            "Rolling Mean: -159.99151480360723\n",
            "Episode failed\n",
            "New Epsilon: 0.43514240105855023\n",
            "Episode: 168; Step: 75; Reward: -100; Total Episode Rewards (thus far): -121.58991630312265\n",
            "Rolling Mean: -159.4267854138942\n",
            "Episode failed\n",
            "New Epsilon: 0.43296668905325747\n",
            "Episode: 169; Step: 87; Reward: -100; Total Episode Rewards (thus far): -192.61910262991518\n",
            "Rolling Mean: -159.90783348948872\n",
            "Episode failed\n",
            "New Epsilon: 0.43080185560799117\n",
            "Episode: 170; Step: 111; Reward: -100; Total Episode Rewards (thus far): -96.65091782534671\n",
            "Rolling Mean: -159.00416326571528\n",
            "Episode failed\n",
            "New Epsilon: 0.42864784632995123\n",
            "Episode: 171; Step: 158; Reward: -100; Total Episode Rewards (thus far): -113.26003072854753\n",
            "Rolling Mean: -158.35987970885375\n",
            "Episode failed\n",
            "New Epsilon: 0.42650460709830146\n",
            "Episode: 172; Step: 197; Reward: -100; Total Episode Rewards (thus far): -88.85940864395397\n",
            "Rolling Mean: -157.39459538850792\n",
            "Episode failed\n",
            "New Epsilon: 0.42437208406280996\n",
            "Episode: 173; Step: 227; Reward: -100; Total Episode Rewards (thus far): -67.00148260190403\n",
            "Rolling Mean: -156.15633356951335\n",
            "Episode failed\n",
            "New Epsilon: 0.4222502236424959\n",
            "Episode: 174; Step: 198; Reward: -100; Total Episode Rewards (thus far): -72.92547998528993\n",
            "Rolling Mean: -155.03159230486168\n",
            "Episode failed\n",
            "New Epsilon: 0.42013897252428345\n",
            "Episode: 175; Step: 222; Reward: -100; Total Episode Rewards (thus far): -14.065226453092691\n",
            "Rolling Mean: -153.15204076017142\n",
            "Episode failed\n",
            "New Epsilon: 0.418038277661662\n",
            "Episode: 176; Step: 195; Reward: -100; Total Episode Rewards (thus far): -67.82703786450847\n",
            "Rolling Mean: -152.02934335364952\n",
            "Episode failed\n",
            "New Epsilon: 0.4159480862733537\n",
            "Episode: 177; Step: 212; Reward: -100; Total Episode Rewards (thus far): -345.6974566007522\n",
            "Rolling Mean: -154.54451365555997\n",
            "Episode failed\n",
            "New Epsilon: 0.41386834584198695\n",
            "Episode: 178; Step: 344; Reward: -100; Total Episode Rewards (thus far): -30.30906243854139\n",
            "Rolling Mean: -152.95175146046998\n",
            "Episode failed\n",
            "New Epsilon: 0.41179900411277703\n",
            "Episode: 179; Step: 116; Reward: -100; Total Episode Rewards (thus far): -342.1017734846097\n",
            "Rolling Mean: -155.34605553672492\n",
            "Episode failed\n",
            "New Epsilon: 0.40974000909221314\n",
            "Episode: 180; Step: 109; Reward: -100; Total Episode Rewards (thus far): -306.999661513221\n",
            "Rolling Mean: -157.24172561143112\n",
            "Episode failed\n",
            "New Epsilon: 0.40769130904675205\n",
            "Episode: 181; Step: 151; Reward: -100; Total Episode Rewards (thus far): -135.86861483103613\n",
            "Rolling Mean: -156.97786004624103\n",
            "Episode failed\n",
            "New Epsilon: 0.4056528525015183\n",
            "Episode: 182; Step: 157; Reward: -100; Total Episode Rewards (thus far): -33.594838008165866\n",
            "Rolling Mean: -155.4731890457767\n",
            "Episode failed\n",
            "New Epsilon: 0.4036245882390107\n",
            "Episode: 183; Step: 266; Reward: -100; Total Episode Rewards (thus far): 56.785089440284196\n",
            "Rolling Mean: -152.91586038931817\n",
            "Episode failed\n",
            "New Epsilon: 0.40160646529781563\n",
            "Episode: 184; Step: 246; Reward: -100; Total Episode Rewards (thus far): -102.35112341580626\n",
            "Rolling Mean: -152.3138992348716\n",
            "Episode failed\n",
            "New Epsilon: 0.3995984329713265\n",
            "Episode: 185; Step: 253; Reward: -100; Total Episode Rewards (thus far): -292.14317135889223\n",
            "Rolling Mean: -153.95894949515417\n",
            "Episode failed\n",
            "New Epsilon: 0.3976004408064699\n",
            "Episode: 186; Step: 111; Reward: -100; Total Episode Rewards (thus far): -244.75630195827495\n",
            "Rolling Mean: -155.01473266333002\n",
            "Episode failed\n",
            "New Epsilon: 0.39561243860243755\n",
            "Episode: 187; Step: 144; Reward: -100; Total Episode Rewards (thus far): -353.25675022498535\n",
            "Rolling Mean: -157.29337654334904\n",
            "Episode failed\n",
            "New Epsilon: 0.3936343764094254\n",
            "Episode: 188; Step: 289; Reward: -100; Total Episode Rewards (thus far): -222.94320056041232\n",
            "Rolling Mean: -158.03939727081567\n",
            "Episode failed\n",
            "New Epsilon: 0.39166620452737827\n",
            "Episode: 189; Step: 112; Reward: -100; Total Episode Rewards (thus far): -106.13701808063739\n",
            "Rolling Mean: -157.45622447092603\n",
            "Episode failed\n",
            "New Epsilon: 0.3897078735047414\n",
            "Episode: 190; Step: 181; Reward: -100; Total Episode Rewards (thus far): -351.63771582306634\n",
            "Rolling Mean: -159.6137965970609\n",
            "Episode failed\n",
            "New Epsilon: 0.3877593341372177\n",
            "Episode: 191; Step: 143; Reward: -100; Total Episode Rewards (thus far): -31.400032306858762\n",
            "Rolling Mean: -158.20485413233342\n",
            "Episode failed\n",
            "New Epsilon: 0.3858205374665316\n",
            "Episode: 192; Step: 194; Reward: -100; Total Episode Rewards (thus far): -58.76975828493675\n",
            "Rolling Mean: -157.1240378731226\n",
            "Episode failed\n",
            "New Epsilon: 0.38389143477919896\n",
            "Episode: 193; Step: 116; Reward: -100; Total Episode Rewards (thus far): -107.57686653548379\n",
            "Rolling Mean: -156.59127258992217\n",
            "Episode failed\n",
            "New Epsilon: 0.38197197760530294\n",
            "Episode: 194; Step: 233; Reward: -100; Total Episode Rewards (thus far): -281.1841473190553\n",
            "Rolling Mean: -157.9167287040619\n",
            "Episode failed\n",
            "New Epsilon: 0.38006211771727644\n",
            "Episode: 195; Step: 187; Reward: -100; Total Episode Rewards (thus far): -576.3247102785383\n",
            "Rolling Mean: -162.32102324695114\n",
            "Episode failed\n",
            "New Epsilon: 0.3781618071286901\n",
            "Episode: 196; Step: 156; Reward: -100; Total Episode Rewards (thus far): -234.93047169362498\n",
            "Rolling Mean: -163.07737166827067\n",
            "Episode failed\n",
            "New Epsilon: 0.37627099809304665\n",
            "Episode: 197; Step: 162; Reward: -100; Total Episode Rewards (thus far): -231.0709528437122\n",
            "Rolling Mean: -163.77833642265665\n",
            "Episode failed\n",
            "New Epsilon: 0.37438964310258144\n",
            "Episode: 198; Step: 146; Reward: -100; Total Episode Rewards (thus far): -142.81327041883685\n",
            "Rolling Mean: -163.56440717771974\n",
            "Episode failed\n",
            "New Epsilon: 0.37251769488706854\n",
            "Episode: 199; Step: 255; Reward: -100; Total Episode Rewards (thus far): -90.68807139799944\n",
            "Rolling Mean: -162.82828257388417\n",
            "Episode failed\n",
            "New Epsilon: 0.3706551064126332\n",
            "Episode: 200; Step: 218; Reward: -100; Total Episode Rewards (thus far): -253.5419722654852\n",
            "Rolling Mean: -163.7354194708002\n",
            "Episode failed\n",
            "New Epsilon: 0.36880183088057006\n",
            "Episode: 201; Step: 150; Reward: -100; Total Episode Rewards (thus far): -127.63640020789211\n",
            "Rolling Mean: -161.45552594308646\n",
            "Episode failed\n",
            "New Epsilon: 0.3669578217261672\n",
            "Episode: 202; Step: 150; Reward: -100; Total Episode Rewards (thus far): -49.449344246067916\n",
            "Rolling Mean: -159.94348085385485\n",
            "Episode failed\n",
            "New Epsilon: 0.36512303261753637\n",
            "Episode: 203; Step: 149; Reward: -100; Total Episode Rewards (thus far): -355.156403839683\n",
            "Rolling Mean: -161.6293873809205\n",
            "Episode failed\n",
            "New Epsilon: 0.3632974174544487\n",
            "Episode: 204; Step: 129; Reward: -100; Total Episode Rewards (thus far): -271.29067675511004\n",
            "Rolling Mean: -163.32890915313035\n",
            "Episode failed\n",
            "New Epsilon: 0.3614809303671765\n",
            "Episode: 205; Step: 175; Reward: -100; Total Episode Rewards (thus far): -371.6424166668561\n",
            "Rolling Mean: -165.264758621408\n",
            "Episode failed\n",
            "New Epsilon: 0.3596735257153406\n",
            "Episode: 206; Step: 177; Reward: -100; Total Episode Rewards (thus far): -238.67992574817484\n",
            "Rolling Mean: -165.20194751618453\n",
            "Episode failed\n",
            "New Epsilon: 0.3578751580867639\n",
            "Episode: 207; Step: 273; Reward: -100; Total Episode Rewards (thus far): -157.5636742620669\n",
            "Rolling Mean: -164.24641496041733\n",
            "Episode failed\n",
            "New Epsilon: 0.3560857822963301\n",
            "Episode: 208; Step: 305; Reward: -100; Total Episode Rewards (thus far): -104.29244670843629\n",
            "Rolling Mean: -159.579985932851\n",
            "Episode failed\n",
            "New Epsilon: 0.3543053533848484\n",
            "Episode: 209; Step: 190; Reward: -100; Total Episode Rewards (thus far): -302.7158655241594\n",
            "Rolling Mean: -159.7817405982241\n",
            "Episode failed\n",
            "New Epsilon: 0.35253382661792415\n",
            "Episode: 210; Step: 100; Reward: -100; Total Episode Rewards (thus far): -361.55634118565524\n",
            "Rolling Mean: -162.37574741084455\n",
            "Episode failed\n",
            "New Epsilon: 0.3507711574848345\n",
            "Episode: 211; Step: 165; Reward: -100; Total Episode Rewards (thus far): -161.59375105685473\n",
            "Rolling Mean: -162.06518203908155\n",
            "Episode failed\n",
            "New Epsilon: 0.34901730169741035\n",
            "Episode: 212; Step: 177; Reward: -100; Total Episode Rewards (thus far): -263.58701937016315\n",
            "Rolling Mean: -164.12319693207948\n",
            "Episode failed\n",
            "New Epsilon: 0.3472722151889233\n",
            "Episode: 213; Step: 184; Reward: -100; Total Episode Rewards (thus far): -255.37609248239949\n",
            "Rolling Mean: -166.30454492996202\n",
            "Episode failed\n",
            "New Epsilon: 0.3455358541129787\n",
            "Episode: 214; Step: 179; Reward: -100; Total Episode Rewards (thus far): -245.5563689763127\n",
            "Rolling Mean: -166.62452175422663\n",
            "Episode failed\n",
            "New Epsilon: 0.3438081748424138\n",
            "Episode: 215; Step: 189; Reward: -100; Total Episode Rewards (thus far): -197.80731323749615\n",
            "Rolling Mean: -163.38295176498258\n",
            "Episode failed\n",
            "New Epsilon: 0.34208913396820173\n",
            "Episode: 216; Step: 164; Reward: -100; Total Episode Rewards (thus far): -236.42239365740232\n",
            "Rolling Mean: -162.19482250646104\n",
            "Episode failed\n",
            "New Epsilon: 0.3403786882983607\n",
            "Episode: 217; Step: 107; Reward: -100; Total Episode Rewards (thus far): -365.40108139270967\n",
            "Rolling Mean: -165.27843141173616\n",
            "Episode failed\n",
            "New Epsilon: 0.3386767948568689\n",
            "Episode: 218; Step: 167; Reward: -100; Total Episode Rewards (thus far): -319.3273536098454\n",
            "Rolling Mean: -166.21706471191618\n",
            "Episode failed\n",
            "New Epsilon: 0.33698341088258454\n",
            "Episode: 219; Step: 196; Reward: -100; Total Episode Rewards (thus far): -83.45747582581886\n",
            "Rolling Mean: -166.75396116505289\n",
            "Episode failed\n",
            "New Epsilon: 0.3352984938281716\n",
            "Episode: 220; Step: 92; Reward: -100; Total Episode Rewards (thus far): -504.9560599626875\n",
            "Rolling Mean: -169.92479159004063\n",
            "Episode failed\n",
            "New Epsilon: 0.33362200135903075\n",
            "Episode: 221; Step: 118; Reward: -100; Total Episode Rewards (thus far): -347.4637415437056\n",
            "Rolling Mean: -173.2607205418646\n",
            "Episode failed\n",
            "New Epsilon: 0.3319538913522356\n",
            "Episode: 222; Step: 235; Reward: -100; Total Episode Rewards (thus far): -91.3885306759029\n",
            "Rolling Mean: -172.46958783118967\n",
            "Episode failed\n",
            "New Epsilon: 0.3302941218954744\n",
            "Episode: 223; Step: 181; Reward: -4.8194926131106675; Total Episode Rewards (thus far): -224.03542815234212"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a16665641eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-7fe24b865e52>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(warm_start, checkpoint_interval)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ba3125189ffe>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_target_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ba3125189ffe>\u001b[0m in \u001b[0;36mcalculate_target_values\u001b[0;34m(self, memory_batch)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mtarget_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1783\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1785\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1786\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 726\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    749\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 751\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3237\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3238\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3239\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "main(warm_start=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "6d9e508f97a44baf9b1b1739b5316710",
            "c6312c505e7440da9ac0062d7dd9fc46",
            "a633a69e80ff42a1b00072e68313b588",
            "585b5543793c4c1e943de76c2b5667b6",
            "8baf3476a2074178bd3a7ac0136d1e8b",
            "91e9b635da724dce9192eaebaf03381e",
            "8c534b31c00740f8bcde5f469b1e9631",
            "eea3f1898e35430d8ba294f70ecc382c"
          ]
        },
        "id": "3OMpUgi7QL5f",
        "outputId": "dafd6acb-72a0-4ab7-f41b-6bb025b2d471"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 345... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d9e508f97a44baf9b1b1739b5316710",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>steps</td><td>▃▃▂▂▄▃▃▂▂▃▃▂▅▃▁▄▆▅▄▃▄▂▁▄▅█▃▅▃▄▅▃▅▃▄▄▄▄▃▅</td></tr><tr><td>total rewards</td><td>▁▆▃▆█▁█▅▆▇▅▅▃▄▆▆▇▆▆▄▅▆▄▆██▆▆▁▁▃▄▃▁▃▂▃▄▂▆</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>222</td></tr><tr><td>steps</td><td>235</td></tr><tr><td>total rewards</td><td>-91.38853</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">chromatic-goat-15</strong>: <a href=\"https://wandb.ai/ethanolx/lunarlander-dqn/runs/2wx5gjih\" target=\"_blank\">https://wandb.ai/ethanolx/lunarlander-dqn/runs/2wx5gjih</a><br/>\n",
              "Find logs at: <code>./wandb/run-20220201_133320-2wx5gjih/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display \n",
        "import io\n",
        "import base64\n",
        "import gym\n",
        "\n",
        "def show_video(env_name):\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = 'video/{}.mp4'.format(env_name)\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_video_of_model(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    vid = video_recorder.VideoRecorder(env, path=\"video/{}.mp4\".format(env_name))\n",
        "    # agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    for i in range(1000):\n",
        "        print(i)\n",
        "        frame = env.render(mode='rgb_array')\n",
        "        vid.capture_frame()\n",
        "        \n",
        "        # action = agent.act(state)\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        if done:\n",
        "            break\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'gym.envs.box2d' has no attribute 'LunarLander'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36016/1662824398.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshow_video_of_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LunarLander-v2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36016/4122891075.py\u001b[0m in \u001b[0;36mshow_video_of_model\u001b[1;34m(env_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshow_video_of_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mvid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoRecorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"video/{}.mp4\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\ethanol\\Documents\\SP\\Current\\Deep Learning (DELE)\\CA2\\.venv\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\ethanol\\Documents\\SP\\Current\\Deep Learning (DELE)\\CA2\\.venv\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Making new env: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\ethanol\\Documents\\SP\\Current\\Deep Learning (DELE)\\CA2\\.venv\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\ethanol\\Documents\\SP\\Current\\Deep Learning (DELE)\\CA2\\.venv\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'gym.envs.box2d' has no attribute 'LunarLander'"
          ]
        }
      ],
      "source": [
        "show_video_of_model('LunarLander-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "dele-part-b.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "585b5543793c4c1e943de76c2b5667b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eea3f1898e35430d8ba294f70ecc382c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c534b31c00740f8bcde5f469b1e9631",
            "value": 1
          }
        },
        "6d9e508f97a44baf9b1b1739b5316710": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a633a69e80ff42a1b00072e68313b588",
              "IPY_MODEL_585b5543793c4c1e943de76c2b5667b6"
            ],
            "layout": "IPY_MODEL_c6312c505e7440da9ac0062d7dd9fc46"
          }
        },
        "8baf3476a2074178bd3a7ac0136d1e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c534b31c00740f8bcde5f469b1e9631": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91e9b635da724dce9192eaebaf03381e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a633a69e80ff42a1b00072e68313b588": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91e9b635da724dce9192eaebaf03381e",
            "placeholder": "​",
            "style": "IPY_MODEL_8baf3476a2074178bd3a7ac0136d1e8b",
            "value": " 0.29MB of 0.29MB uploaded (0.00MB deduped)\r"
          }
        },
        "c6312c505e7440da9ac0062d7dd9fc46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eea3f1898e35430d8ba294f70ecc382c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
