{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7f4f28d79d194d6a96b267442c04c080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7d292c5eba594f4db2013d3faa9b2d42",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f0a2b4f045c64187a88c35895ddbd89e",
              "IPY_MODEL_3b5ce00d2af2465791891c72c8b5ad5a"
            ]
          }
        },
        "7d292c5eba594f4db2013d3faa9b2d42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0a2b4f045c64187a88c35895ddbd89e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_d5faf472d59c49c8abbab51ea1758c40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.49MB of 0.49MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2bf4fbc9cc544a1eb5fdbd4549cb9e60"
          }
        },
        "3b5ce00d2af2465791891c72c8b5ad5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b4f1c9b95a624a609ab133622a22ab4e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_62bd10bad5854f4e87820b40b4295d84"
          }
        },
        "d5faf472d59c49c8abbab51ea1758c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2bf4fbc9cc544a1eb5fdbd4549cb9e60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4f1c9b95a624a609ab133622a22ab4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "62bd10bad5854f4e87820b40b4295d84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install wandb box2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuEtCrIIeXIn",
        "outputId": "0b6413ca-56ba-46f2-86be-7a35aba58f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting box2d\n",
            "  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 18.2 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 36.2 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.4-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[K     |████████████████████████████████| 143 kB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=4751dcdf04fd5920d0903cfcea7cd9be2d52385122030e12195ca7ffb0c1eaff\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb, box2d\n",
            "Successfully installed GitPython-3.1.26 box2d-2.3.10 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.4 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.10 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install External Dependencies"
      ],
      "metadata": {
        "id": "YkR0L9YCrhWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MtUank4U2HZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbc80f8-860b-402d-8450-f98647da2110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Dependencies"
      ],
      "metadata": {
        "id": "aEkEPLdbrsiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Neural Networks\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
        "from tensorflow.keras.regularizers import *\n",
        "\n",
        "# Reinforcement Learning\n",
        "import gym\n",
        "\n",
        "# Model Tracking\n",
        "import wandb\n",
        "\n",
        "# Data Structures\n",
        "from collections import deque\n",
        "\n",
        "# Miscellaneous\n",
        "import os\n",
        "import pickle"
      ],
      "metadata": {
        "id": "rsp5j7eJh_2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ARTIFACT_DIRECTORY = 'drive/MyDrive/rl/dqn6/'\n",
        "ENV = gym.make('LunarLander-v2')\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "AGENT_CONFIG = {\n",
        "    'batch_size': 128,\n",
        "    'epsilon': 1.0,\n",
        "    'min_epsilon': 0.01,\n",
        "    'epsilon_decay': 0.95,\n",
        "    'alpha': 0.001,\n",
        "    'gamma': 0.95,\n",
        "    'tau': 0.8,\n",
        "    'regularization': 0.002\n",
        "}"
      ],
      "metadata": {
        "id": "6oPZkoeYuRVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_str = '_'.join((f'{k}={v}' for k, v in AGENT_CONFIG.items())) + '.log'\n",
        "config_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cq6i_ypXyfPK",
        "outputId": "948369a3-ccaf-46b2-aa28-ff93a87be370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'batch_size=128_epsilon=1.0_min_epsilon=0.01_epsilon_decay=0.95_alpha=0.001_gamma=0.95_tau=0.8_regularization=0.002.log'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.login()\n",
        "wandb.init(project=\"lunarlander-dqn\",\n",
        "           entity=\"ethanolx\",\n",
        "           config=AGENT_CONFIG)"
      ],
      "metadata": {
        "id": "PpFjcOX4kUhb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "9eb18b43-cb4c-403d-9920-7af5b62ee2a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/ethanolx/lunarlander-dqn/runs/2iffdnrj\" target=\"_blank\">hearty-disco-30</a></strong> to <a href=\"https://wandb.ai/ethanolx/lunarlander-dqn\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fe981a18410>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/ethanolx/lunarlander-dqn/runs/2iffdnrj?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ENV.observation_space.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9gprhW_v8NY",
        "outputId": "e24e36da-eb85-4847-e647-50d979bed338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8,)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ENV.action_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQmlXKgiv_AB",
        "outputId": "6db5211e-1518-4744-db7c-b9479dd9c411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_length: int):\n",
        "        self.max_length = max_length\n",
        "        self.buffer = [None] * self.max_length\n",
        "        self.pointer = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, memory):\n",
        "        if self.pointer >= self.max_length:\n",
        "            self.pointer = 0\n",
        "        elif self.size < self.max_length:\n",
        "            self.size += 1\n",
        "        self.buffer[self.pointer] = memory\n",
        "        self.pointer += 1\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        return [self.buffer[i] for i in np.random.randint(0, self.size, batch_size)]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.buffer[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.size"
      ],
      "metadata": {
        "id": "McuFI9xEhH2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardTracker(deque):\n",
        "    def __init__(self, max_length: int):\n",
        "        super().__init__(maxlen=max_length)\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def log(self, total_episodic_rewards):\n",
        "        self.append(total_episodic_rewards)\n",
        "    \n",
        "    def get_rolling_mean(self):\n",
        "        base = min(self.__len__(), self.max_length)\n",
        "        return sum((r for r in self)) / float(base)"
      ],
      "metadata": {
        "id": "aa2Qwz3hvQvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_huber_loss(mask_value, clip_delta):\n",
        "    def f(y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        cond  = tf.abs(error) < clip_delta\n",
        "        mask_true = tf.cast(tf.not_equal(y_true, mask_value), tf.float32)\n",
        "        masked_squared_error = 0.5 * tf.square(mask_true * (y_true - y_pred))\n",
        "        linear_loss  = mask_true * (clip_delta * tf.abs(error) - 0.5 * (clip_delta ** 2))\n",
        "        huber_loss = tf.where(cond, masked_squared_error, linear_loss)\n",
        "        return tf.reduce_sum(huber_loss) / tf.reduce_sum(mask_true)\n",
        "    f.__name__ = 'masked_huber_loss'\n",
        "    return f"
      ],
      "metadata": {
        "id": "nkket1jdpXfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DQN(input_shape, output_shape, alpha, regularization: float = 0.001):\n",
        "    model = Sequential(layers=[\n",
        "        Dense(128, input_shape=input_shape, activation='relu', kernel_regularizer=l2(regularization)),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(regularization)),\n",
        "        Dense(output_shape, kernel_regularizer=l2(regularization))\n",
        "    ])\n",
        "    model.compile(loss=masked_huber_loss(0.0, 1.0), optimizer=Adam(learning_rate=alpha))\n",
        "    return model"
      ],
      "metadata": {
        "id": "Xq_AujVYjyH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def fill_replay_buffer(max_length: int, episodes: int, timesteps: int):\n",
        "#     replay_buffer = ReplayBuffer(max_length=max_length)\n",
        "#     for episode in range(episodes):\n",
        "#         state = ENV.reset()\n",
        "#         while True:\n",
        "#             action = ENV.action_space.sample()\n",
        "#             new_state, reward, done, _ = ENV.step(action)\n",
        "\n",
        "#             replay_buffer.append((state, action, reward, new_state, done))\n",
        "\n",
        "#             if done:\n",
        "#                 break\n",
        "            \n",
        "#             state = new_state\n",
        "\n",
        "#     print(len(replay_buffer))\n",
        "#     return replay_buffer"
      ],
      "metadata": {
        "id": "Tblr4YSjk3Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FileLogger:\n",
        "    def __init__(self, log_file: str, *args, sep: str=';'):\n",
        "        self.log_file = log_file\n",
        "        self.sep = sep\n",
        "        self.n_args = len(args)\n",
        "        if not os.path.exists(log_file):\n",
        "            self.setup(args)\n",
        "    \n",
        "    def setup(self, args):\n",
        "        header = self.sep.join(args)\n",
        "        with open(self.log_file, mode='w') as f:\n",
        "            f.write(header)\n",
        "            f.write('\\n')\n",
        "\n",
        "    def log(self, **kwargs):\n",
        "        assert len(kwargs) == self.n_args\n",
        "        header = ''\n",
        "        with open(self.log_file, mode='r') as f:\n",
        "            header = f.readlines()[0].replace('\\n', '')\n",
        "        params = header.split(self.sep)\n",
        "        values = (str(kwargs[p.lower()]) for p in params)\n",
        "        entry = self.sep.join(values)\n",
        "        with open(self.log_file, mode='a+') as f:\n",
        "            f.write(entry)\n",
        "            f.write('\\n')\n",
        "        return self\n",
        "    \n",
        "    def get_logs(self):\n",
        "        df = pd.read_csv(self.log_file, header=0, sep=self.sep)\n",
        "        past_rewards = df['Total_Rewards']\n",
        "        return past_rewards"
      ],
      "metadata": {
        "id": "H-lP4_VRj6tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 batch_size: int,\n",
        "                 alpha: float,\n",
        "                 epsilon: float,\n",
        "                 min_epsilon: float,\n",
        "                 epsilon_decay: float,\n",
        "                 gamma: float,\n",
        "                 tau: float,\n",
        "                 regularization: float,\n",
        "                 max_memory_length: int = 250_000,\n",
        "                 checkpoint_interval: int = 10,\n",
        "                 log_file: str = 'progress.log'):\n",
        "        self.env = env\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.replay_buffer = ReplayBuffer(max_length = max_memory_length)\n",
        "        self.model = DQN(input_shape=env.observation_space.shape,\n",
        "                         output_shape=env.action_space.n,\n",
        "                         alpha=alpha,\n",
        "                         regularization=regularization)\n",
        "        self.target_model = DQN(input_shape=env.observation_space.shape,\n",
        "                         output_shape=env.action_space.n,\n",
        "                         alpha=alpha,\n",
        "                         regularization=regularization)\n",
        "        self.file_logger = FileLogger(ARTIFACT_DIRECTORY + log_file, 'Episode', 'Steps', 'Total_Rewards')\n",
        "        self.checkpoint_interval = checkpoint_interval\n",
        "    \n",
        "    def checkpoint(self, episode, steps, total_rewards):\n",
        "        self.file_logger.log(episode=episode, steps=steps, total_rewards=total_rewards)\n",
        "        if episode % self.checkpoint_interval == 0:\n",
        "            self.model.save(ARTIFACT_DIRECTORY + f'{episode}.h5')\n",
        "            with open(f'{ARTIFACT_DIRECTORY}replay_buffer-{episode}.p', 'wb') as saved_buffer:\n",
        "                pickle.dump(self.replay_buffer, saved_buffer)\n",
        "    \n",
        "    def act(self, state):\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        return np.argmax(self.model.predict(state.reshape(1, -1))[0])\n",
        "    \n",
        "    def calculate_target_values(self, memory_batch):\n",
        "        states = []\n",
        "        next_states = []\n",
        "        for sample in memory_batch:\n",
        "            states.append(sample[0])\n",
        "            next_states.append(sample[3])\n",
        "        states = np.array(states)\n",
        "        next_states = np.array(next_states)\n",
        "\n",
        "        q_values = self.model.predict(states)\n",
        "        target_q_values = self.target_model.predict(states)\n",
        "\n",
        "        q_values_next_state = self.model.predict(next_states)\n",
        "        target_q_values_next_state = self.target_model.predict(next_states)\n",
        "\n",
        "        targets = []\n",
        "        for index, (s, a, r, s_, d) in enumerate(memory_batch):\n",
        "            best_action = np.argmax(q_values_next_state[index])\n",
        "            best_action_next_state_q_value = target_q_values_next_state[index][best_action]\n",
        "            \n",
        "            target_vector = [0, 0, 0, 0]\n",
        "            target_vector[a] = r if d else r + self.gamma * best_action_next_state_q_value\n",
        "            targets.append(target_vector)\n",
        "\n",
        "        return states, np.array(targets)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay_buffer) >= self.batch_size:\n",
        "            samples = self.replay_buffer.sample(batch_size=self.batch_size)\n",
        "            states, targets = self.calculate_target_values(samples)\n",
        "            self.model.fit(states, targets, epochs=1, batch_size=targets.shape[0], verbose=0)\n",
        "            # self.decay_epsilon()\n",
        "    \n",
        "    def transfer_weights(self):\n",
        "        policy_weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        if self.tau < 1.0:\n",
        "            for i, weights in enumerate(policy_weights):\n",
        "                target_weights[i] = weights * self.tau + target_weights[i] * (1 - self.tau)\n",
        "        self.target_model.set_weights(target_weights)\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)"
      ],
      "metadata": {
        "id": "kqogW6_Qj3n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(warm_start: int = 0):\n",
        "    episodes = 1000\n",
        "    max_steps_per_episode = 1000\n",
        "    target_model_train_interval = 1\n",
        "    reward_progress = RewardTracker(max_length=100)\n",
        "    dqn_agent = Agent(env=ENV, **AGENT_CONFIG, log_file=config_str)\n",
        "    if warm_start >= 1:\n",
        "        print(f'Resuming from last episode: {warm_start}')\n",
        "        dqn_agent.epsilon = max(dqn_agent.min_epsilon, dqn_agent.epsilon_decay ** warm_start)\n",
        "        dqn_agent.target_model = load_model(f'{ARTIFACT_DIRECTORY}{warm_start}.h5',\n",
        "                                            custom_objects={'masked_huber_loss': masked_huber_loss(0.0, 1.0)})\n",
        "        dqn_agent.model = load_model(f'{ARTIFACT_DIRECTORY}{warm_start}.h5',\n",
        "                                            custom_objects={'masked_huber_loss': masked_huber_loss(0.0, 1.0)})\n",
        "        with open(f'{ARTIFACT_DIRECTORY}replay_buffer-{warm_start}.p', 'rb') as saved_buffer:\n",
        "            dqn_agent.replay_buffer = pickle.load(saved_buffer)\n",
        "        for r in dqn_agent.file_logger.get_logs():\n",
        "            reward_progress.append(r)\n",
        "    for episode in range(warm_start + 1, episodes + warm_start + 1):\n",
        "        cur_state = ENV.reset()\n",
        "        total_episode_rewards = 0\n",
        "        for step in range(max_steps_per_episode):\n",
        "            action = dqn_agent.act(cur_state)\n",
        "            new_state, reward, done, _ = ENV.step(action)\n",
        "\n",
        "            total_episode_rewards += reward\n",
        "            print(f'\\rEpisode: {episode}; Step: {step}; Reward: {reward}; Total Episode Rewards (thus far): {total_episode_rewards}', end='')\n",
        "\n",
        "            dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
        "            dqn_agent.learn()\n",
        "            cur_state = new_state\n",
        "            if done or total_episode_rewards < -300:\n",
        "                break\n",
        "        else:\n",
        "            print(f'\\nFailed to complete episode {episode} within {max_steps_per_episode} steps')\n",
        "        print()\n",
        "        if episode % target_model_train_interval == 0:\n",
        "            dqn_agent.transfer_weights()\n",
        "        reward_progress.append(total_episode_rewards)\n",
        "        print(f'Rolling Mean: {reward_progress.get_rolling_mean()}')\n",
        "        dqn_agent.checkpoint(episode=episode, steps=step, total_rewards=total_episode_rewards)\n",
        "        dqn_agent.decay_epsilon()\n",
        "        if total_episode_rewards < 200:\n",
        "            print(\"Episode failed\")\n",
        "        else:\n",
        "            print(f\"Completed in {episode} episodes\")\n",
        "        print(f'New Epsilon: {dqn_agent.epsilon}')\n",
        "        wandb.log({\n",
        "            'episode': episode,\n",
        "            'steps': step,\n",
        "            'total rewards': total_episode_rewards\n",
        "        })"
      ],
      "metadata": {
        "id": "HH_nwv1rvBWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(warm_start=0)"
      ],
      "metadata": {
        "id": "7-GGNyin0e2A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a3a69f8-ca2b-4149-8135-6825d23099eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1; Step: 103; Reward: -4.000324336028; Total Episode Rewards (thus far): -303.1313142798565\n",
            "Rolling Mean: -303.1313142798565\n",
            "Episode failed\n",
            "New Epsilon: 0.95\n",
            "Episode: 2; Step: 59; Reward: -100; Total Episode Rewards (thus far): -97.14827819355696\n",
            "Rolling Mean: -200.13979623670673\n",
            "Episode failed\n",
            "New Epsilon: 0.9025\n",
            "Episode: 3; Step: 77; Reward: -100; Total Episode Rewards (thus far): -119.1424031512761\n",
            "Rolling Mean: -173.14066520822985\n",
            "Episode failed\n",
            "New Epsilon: 0.8573749999999999\n",
            "Episode: 4; Step: 115; Reward: -100; Total Episode Rewards (thus far): -133.5400980949639\n",
            "Rolling Mean: -163.24052342991337\n",
            "Episode failed\n",
            "New Epsilon: 0.8145062499999999\n",
            "Episode: 5; Step: 135; Reward: -100; Total Episode Rewards (thus far): -82.6656735150271\n",
            "Rolling Mean: -147.1255534469361\n",
            "Episode failed\n",
            "New Epsilon: 0.7737809374999999\n",
            "Episode: 6; Step: 110; Reward: -100; Total Episode Rewards (thus far): -102.41459619907285\n",
            "Rolling Mean: -139.6737272389589\n",
            "Episode failed\n",
            "New Epsilon: 0.7350918906249998\n",
            "Episode: 7; Step: 88; Reward: -100; Total Episode Rewards (thus far): -192.53258080094338\n",
            "Rolling Mean: -147.22499203352814\n",
            "Episode failed\n",
            "New Epsilon: 0.6983372960937497\n",
            "Episode: 8; Step: 89; Reward: -100; Total Episode Rewards (thus far): -255.2103477775492\n",
            "Rolling Mean: -160.72316150153077\n",
            "Episode failed\n",
            "New Epsilon: 0.6634204312890623\n",
            "Episode: 9; Step: 114; Reward: -100; Total Episode Rewards (thus far): -289.8967624127073\n",
            "Rolling Mean: -175.07578382499483\n",
            "Episode failed\n",
            "New Epsilon: 0.6302494097246091\n",
            "Episode: 10; Step: 117; Reward: -100; Total Episode Rewards (thus far): -206.1299012277557\n",
            "Rolling Mean: -178.1811955652709\n",
            "Episode failed\n",
            "New Epsilon: 0.5987369392383786\n",
            "Episode: 11; Step: 156; Reward: -100; Total Episode Rewards (thus far): 43.6721400764595\n",
            "Rolling Mean: -158.0127105069318\n",
            "Episode failed\n",
            "New Epsilon: 0.5688000922764596\n",
            "Episode: 12; Step: 224; Reward: -100; Total Episode Rewards (thus far): -190.47688008594395\n",
            "Rolling Mean: -160.71805797184948\n",
            "Episode failed\n",
            "New Epsilon: 0.5403600876626365\n",
            "Episode: 13; Step: 120; Reward: -100; Total Episode Rewards (thus far): -42.02993632548724\n",
            "Rolling Mean: -151.58820246059082\n",
            "Episode failed\n",
            "New Epsilon: 0.5133420832795047\n",
            "Episode: 14; Step: 242; Reward: -100; Total Episode Rewards (thus far): -94.74532718487906\n",
            "Rolling Mean: -147.5279970837543\n",
            "Episode failed\n",
            "New Epsilon: 0.48767497911552943\n",
            "Episode: 15; Step: 93; Reward: -100; Total Episode Rewards (thus far): -22.359036398971952\n",
            "Rolling Mean: -139.1833997047688\n",
            "Episode failed\n",
            "New Epsilon: 0.46329123015975293\n",
            "Episode: 16; Step: 725; Reward: -100; Total Episode Rewards (thus far): -179.70764375434638\n",
            "Rolling Mean: -141.7161649578674\n",
            "Episode failed\n",
            "New Epsilon: 0.44012666865176525\n",
            "Episode: 17; Step: 525; Reward: -100; Total Episode Rewards (thus far): -83.62011649174019\n",
            "Rolling Mean: -138.29875034221286\n",
            "Episode failed\n",
            "New Epsilon: 0.41812033521917696\n",
            "Episode: 18; Step: 116; Reward: -100; Total Episode Rewards (thus far): -46.132701748613286\n",
            "Rolling Mean: -133.1784143092351\n",
            "Episode failed\n",
            "New Epsilon: 0.3972143184582181\n",
            "Episode: 19; Step: 281; Reward: -100; Total Episode Rewards (thus far): 19.88045673819407\n",
            "Rolling Mean: -125.12268425410726\n",
            "Episode failed\n",
            "New Epsilon: 0.37735360253530714\n",
            "Episode: 20; Step: 237; Reward: -100; Total Episode Rewards (thus far): -79.22023196205762\n",
            "Rolling Mean: -122.82756163950478\n",
            "Episode failed\n",
            "New Epsilon: 0.35848592240854177\n",
            "Episode: 21; Step: 702; Reward: -100; Total Episode Rewards (thus far): -185.77554560203356\n",
            "Rolling Mean: -125.82508468533949\n",
            "Episode failed\n",
            "New Epsilon: 0.34056162628811465\n",
            "Episode: 22; Step: 999; Reward: -0.3748573755330984; Total Episode Rewards (thus far): 117.1677119207487\n",
            "Rolling Mean: -114.77995756688092\n",
            "Episode failed\n",
            "New Epsilon: 0.3235335449737089\n",
            "Episode: 23; Step: 999; Reward: 0.6432309545121953; Total Episode Rewards (thus far): -86.27544826510065\n",
            "Rolling Mean: -113.54063107549918\n",
            "Episode failed\n",
            "New Epsilon: 0.30735686772502346\n",
            "Episode: 24; Step: 318; Reward: -100; Total Episode Rewards (thus far): -37.1236601248042\n",
            "Rolling Mean: -110.35659061922023\n",
            "Episode failed\n",
            "New Epsilon: 0.2919890243387723\n",
            "Episode: 25; Step: 999; Reward: 1.35427806608632; Total Episode Rewards (thus far): -65.29718982712035\n",
            "Rolling Mean: -108.55421458753622\n",
            "Episode failed\n",
            "New Epsilon: 0.27738957312183365\n",
            "Episode: 26; Step: 999; Reward: -0.18445337604908046; Total Episode Rewards (thus far): -97.05910554732702\n",
            "Rolling Mean: -108.11209500906664\n",
            "Episode failed\n",
            "New Epsilon: 0.263520094465742\n",
            "Episode: 27; Step: 598; Reward: -100; Total Episode Rewards (thus far): -280.2024797937147\n",
            "Rolling Mean: -114.48581296405361\n",
            "Episode failed\n",
            "New Epsilon: 0.25034408974245487\n",
            "Episode: 28; Step: 999; Reward: -0.9441954246660345; Total Episode Rewards (thus far): -45.89133676551387\n",
            "Rolling Mean: -112.0360102426772\n",
            "Episode failed\n",
            "New Epsilon: 0.2378268852553321\n",
            "Episode: 29; Step: 176; Reward: -100; Total Episode Rewards (thus far): -27.04898497272258\n",
            "Rolling Mean: -109.10542316440291\n",
            "Episode failed\n",
            "New Epsilon: 0.2259355409925655\n",
            "Episode: 30; Step: 999; Reward: 1.830148855497623; Total Episode Rewards (thus far): -85.96511517780917\n",
            "Rolling Mean: -108.33407956484979\n",
            "Episode failed\n",
            "New Epsilon: 0.2146387639429372\n",
            "Episode: 31; Step: 999; Reward: 1.4324601307808222; Total Episode Rewards (thus far): -93.86382714250753\n",
            "Rolling Mean: -107.8672972286452\n",
            "Episode failed\n",
            "New Epsilon: 0.20390682574579033\n",
            "Episode: 32; Step: 999; Reward: -0.08072791982253874; Total Episode Rewards (thus far): -71.7702275750319\n",
            "Rolling Mean: -106.73926380196978\n",
            "Episode failed\n",
            "New Epsilon: 0.1937114844585008\n",
            "Episode: 33; Step: 999; Reward: -2.3811578768416837; Total Episode Rewards (thus far): -141.67034290955073\n",
            "Rolling Mean: -107.79778135068435\n",
            "Episode failed\n",
            "New Epsilon: 0.18402591023557577\n",
            "Episode: 34; Step: 999; Reward: -0.41477346972251894; Total Episode Rewards (thus far): -95.54746177511579\n",
            "Rolling Mean: -107.43747783375586\n",
            "Episode failed\n",
            "New Epsilon: 0.17482461472379698\n",
            "Episode: 35; Step: 999; Reward: 0.714864862046096; Total Episode Rewards (thus far): -41.31534497489171\n",
            "Rolling Mean: -105.5482740377883\n",
            "Episode failed\n",
            "New Epsilon: 0.16608338398760714\n",
            "Episode: 36; Step: 999; Reward: 0.9026179732998514; Total Episode Rewards (thus far): -112.13781987558036\n",
            "Rolling Mean: -105.73131697772698\n",
            "Episode failed\n",
            "New Epsilon: 0.15777921478822676\n",
            "Episode: 37; Step: 999; Reward: -0.4726570327951265; Total Episode Rewards (thus far): -97.96783752599491\n",
            "Rolling Mean: -105.52149320876124\n",
            "Episode failed\n",
            "New Epsilon: 0.14989025404881542\n",
            "Episode: 38; Step: 999; Reward: -1.630395870331056; Total Episode Rewards (thus far): -124.84135180555548\n",
            "Rolling Mean: -106.02991054025583\n",
            "Episode failed\n",
            "New Epsilon: 0.14239574134637464\n",
            "Episode: 39; Step: 999; Reward: -1.6792117244828404; Total Episode Rewards (thus far): -58.30357884828521\n",
            "Rolling Mean: -104.80615844558992\n",
            "Episode failed\n",
            "New Epsilon: 0.1352759542790559\n",
            "Episode: 40; Step: 999; Reward: -2.95241767398237; Total Episode Rewards (thus far): -143.23758098023634\n",
            "Rolling Mean: -105.76694400895607\n",
            "Episode failed\n",
            "New Epsilon: 0.1285121565651031\n",
            "Episode: 41; Step: 999; Reward: -0.7703310161804098; Total Episode Rewards (thus far): -34.33468422850826\n",
            "Rolling Mean: -104.02469377040858\n",
            "Episode failed\n",
            "New Epsilon: 0.12208654873684793\n",
            "Episode: 42; Step: 999; Reward: -1.3216109690415425; Total Episode Rewards (thus far): -83.56772686973727\n",
            "Rolling Mean: -103.5376231299164\n",
            "Episode failed\n",
            "New Epsilon: 0.11598222130000553\n",
            "Episode: 43; Step: 999; Reward: -2.2185493261879103; Total Episode Rewards (thus far): -99.98592233275836\n",
            "Rolling Mean: -103.45502543695923\n",
            "Episode failed\n",
            "New Epsilon: 0.11018311023500525\n",
            "Episode: 44; Step: 999; Reward: -1.3594148130341626; Total Episode Rewards (thus far): -160.53583533869303\n",
            "Rolling Mean: -104.75231657108954\n",
            "Episode failed\n",
            "New Epsilon: 0.10467395472325498\n",
            "Episode: 45; Step: 999; Reward: 0.07122173922520575; Total Episode Rewards (thus far): -98.57015431696266\n",
            "Rolling Mean: -104.6149351876645\n",
            "Episode failed\n",
            "New Epsilon: 0.09944025698709223\n",
            "Episode: 46; Step: 999; Reward: -0.45763947743782296; Total Episode Rewards (thus far): -163.48044900584986\n",
            "Rolling Mean: -105.89462027066853\n",
            "Episode failed\n",
            "New Epsilon: 0.09446824413773762\n",
            "Episode: 47; Step: 999; Reward: 0.1511212675522245; Total Episode Rewards (thus far): -80.48479034230509\n",
            "Rolling Mean: -105.35398559134164\n",
            "Episode failed\n",
            "New Epsilon: 0.08974483193085074\n",
            "Episode: 48; Step: 999; Reward: -0.6363016657917513; Total Episode Rewards (thus far): -123.7391802118785\n",
            "Rolling Mean: -105.73701047926949\n",
            "Episode failed\n",
            "New Epsilon: 0.0852575903343082\n",
            "Episode: 49; Step: 999; Reward: -1.7431830726507258; Total Episode Rewards (thus far): -150.76996228159643\n",
            "Rolling Mean: -106.65605031197003\n",
            "Episode failed\n",
            "New Epsilon: 0.08099471081759278\n",
            "Episode: 50; Step: 999; Reward: -0.733235781213665; Total Episode Rewards (thus far): -119.04770428581114\n",
            "Rolling Mean: -106.90388339144685\n",
            "Episode failed\n",
            "New Epsilon: 0.07694497527671314\n",
            "Episode: 51; Step: 999; Reward: 0.8545981416547874; Total Episode Rewards (thus far): -104.60814940627904\n",
            "Rolling Mean: -106.85886899958082\n",
            "Episode failed\n",
            "New Epsilon: 0.07309772651287748\n",
            "Episode: 52; Step: 635; Reward: 0.396129582640026; Total Episode Rewards (thus far): -70.93208431322392"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-260ba2cca6eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-aeaf4e47175f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(warm_start)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal_episode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps_per_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e0d5048753fd>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_target_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1766\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     dataset = dataset.map(\n\u001b[0;32m--> 360\u001b[0;31m         grab_batch, num_parallel_calls=tf.data.AUTOTUNE)\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2010\u001b[0m           \u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m           \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2012\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   2013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2014\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5504\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5505\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   5506\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5507\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"default\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   4531\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4533\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4534\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4535\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3243\u001b[0m     \"\"\"\n\u001b[1;32m   3244\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3245\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3246\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3247\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3210\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3557\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3558\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3400\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3401\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3402\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3403\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4508\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   4509\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4510\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4511\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4444\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4445\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4446\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4447\u001b[0m         six.reraise(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m   \"\"\"\n\u001b[0;32m--> 440\u001b[0;31m   \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m_type_spec_from_value\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    873\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;31m# Note: we do not include Tensor names when constructing TypeSpecs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompositeTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \"\"\"\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_val\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_api_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_c_api_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0mc_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     shape_vec, unknown_shape = pywrap_tf_session.TF_GraphGetTensorShapeHelper(\n\u001b[0;32m--> 506\u001b[0;31m         c_graph, self._as_tf_output())\n\u001b[0m\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munknown_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munknown_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "EzhNIATpeslz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "7f4f28d79d194d6a96b267442c04c080",
            "7d292c5eba594f4db2013d3faa9b2d42",
            "f0a2b4f045c64187a88c35895ddbd89e",
            "3b5ce00d2af2465791891c72c8b5ad5a",
            "d5faf472d59c49c8abbab51ea1758c40",
            "2bf4fbc9cc544a1eb5fdbd4549cb9e60",
            "b4f1c9b95a624a609ab133622a22ab4e",
            "62bd10bad5854f4e87820b40b4295d84"
          ]
        },
        "outputId": "fca3b051-b7c6-49c4-92b0-a84dc8ee7b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 350... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f4f28d79d194d6a96b267442c04c080",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▂▂▁▁▆▄▁▂▆█▃██▅▂█████████████████</td></tr><tr><td>total rewards</td><td>▁▄▄▄▄▃▂▁▇▃▅▆▃▅▅▅▃█▅▅▄▁▆▅▄▄▄▅▄▄▅▄▅▄▃▄▅▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>51</td></tr><tr><td>steps</td><td>999</td></tr><tr><td>total rewards</td><td>-104.60815</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">hearty-disco-30</strong>: <a href=\"https://wandb.ai/ethanolx/lunarlander-dqn/runs/2iffdnrj\" target=\"_blank\">https://wandb.ai/ethanolx/lunarlander-dqn/runs/2iffdnrj</a><br/>\n",
              "Find logs at: <code>./wandb/run-20220210_071719-2iffdnrj/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# !apt-get install cmake > /dev/null 2>&1\n",
        "# !pip install --upgrade setuptools 2>&1\n",
        "# !pip install ez-setup > /dev/null 2>&1\n",
        "# !pip install pyvirtualdisplay\n",
        "# !pip install box2d-py"
      ],
      "metadata": {
        "id": "KDDfdWNb_NoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "jN0m_SBdCFqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r drive/MyDrive/rl/dqn5/video\n",
        "# !cd drive/MyDrive/rl;xvfb-run -a -s \"-screen 0 640x480x24\" python make_video.py"
      ],
      "metadata": {
        "id": "Vqbi_BdmAejM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}